{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9619b018-f7a0-4d13-90b4-f769a8906003",
    "_uuid": "cac9915711fdcec7e8eb6431fd8dcae21649b6aa"
   },
   "source": [
    "**Building a strong image classification model from less data**\n",
    "\n",
    "The implementation is a slight variation of the one in https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "\n",
    "Mainly, in this kernel , the method flow(x,y) is used whereas, in the above gist, method flow_from_directory(directory) is used.\n",
    "For more info, you can refer https://keras.io/preprocessing/image/\n",
    "\n",
    "The change is made to have an appropriate kernel to deal with the way data is structured in kaggle. Appropriate changes in other parts of the source code is also done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e10726c1-a487-4d89-bf8f-7ff532592440",
    "_uuid": "16fefbb9857aead05fe80ecbf3ffbabfae4fca22"
   },
   "source": [
    "**Perform the necessary imports.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4f4ac6a4-708c-46ab-8269-e01978300bde",
    "_uuid": "49532c85c74dee05663bd7eda324d3df493c60ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, re, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models       import Model\n",
    "import tensorflow as tf\n",
    "from keras.models       import Sequential\n",
    "\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5fa55078-4136-47a1-9914-1cac54eec440",
    "_uuid": "d57ed5db51e8c287e62a8b40d264c42ca64a9ecf"
   },
   "source": [
    "**Data dimensions and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "25980d83-1f66-420b-a1dc-501699c3d707",
    "_uuid": "2eb716c4e9ad7d0bd7293f0f156b5a751263b800"
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'train2/'\n",
    "TEST_DIR = 'test/'\n",
    "train_images_dogs_cats = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\n",
    "test_images_dogs_cats = [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n",
    "NO_EPOCHS=5\n",
    "RESNET_WEIGHTS_PATH = '../input/keras-pretrained-models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7fd765e1-e4e1-4b59-b7b2-33336afd4848",
    "_uuid": "505895b51eb3ed5a1dee521c9adc0864639d0b96"
   },
   "source": [
    "\n",
    "**Helper function to sort the image files based on the numeric value in each file name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "ab823826f480c084baf6029949137eb19b1c018b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images_dogs_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4c8b492-315c-4c38-9b1e-32c05e368027",
    "_uuid": "11a7c4072cdedd8f5b79d4bfcec3d1f31d448404"
   },
   "source": [
    "**Sort the traning set. Use 1300 images each of cats and dogs instead of all 25000 to speed up the learning process.**\n",
    "\n",
    "**Sort the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dogs_cats = train_images_dogs_cats[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d9bb401814a6f96751ad68c7974e42c33bcfd98a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "58d15bea-7674-46ff-ba0d-af6a920590be",
    "_uuid": "5d3a332070aadd998dcb3ac506f086d1cbe37b20"
   },
   "source": [
    "**Now the images have to be represented in numbers. For this, using the openCV library read and resize the image.  **\n",
    "\n",
    "**Generate labels for the supervised learning set.**\n",
    "\n",
    "**Below is the helper function to do so.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(list_of_images):\n",
    "    \"\"\"\n",
    "    Returns two arrays: \n",
    "        x is an array of resized images\n",
    "        y is an array of labels\n",
    "    \"\"\"\n",
    "    x = [] # images as arrays\n",
    "    y = [] # labels\n",
    "    \n",
    "    for image in (list_of_images):\n",
    "        x.append(cv2.resize(cv2.imread(image), (224,224), interpolation=cv2.INTER_CUBIC))\n",
    "    \n",
    "    for i in list_of_images:\n",
    "        if 'dog' in i:\n",
    "            y.append(1)\n",
    "        elif 'cat' in i:\n",
    "            y.append(0)\n",
    "        #else:\n",
    "            #print('neither cat nor dog name present in images')\n",
    "            \n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9add14aa-3cd6-4160-a3f3-20a2f6b61af9",
    "_uuid": "0111553c02dd7e4d622aeb65c3d307038f32f000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "X ,Y= prepare_data(train_images_dogs_cats)\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3c5b384bc91a4abe708c2d21787352c552434097"
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "66fcb69be47911c24423cf0bb65050cd4937bac6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([500, 500], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "34f900bcb923c5e32eac97878763efefbe8c880a"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y1 = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "6a94930d-9c33-4361-ba3d-ef1f29707bce",
    "_uuid": "26eea9b27bcbf6a0ca3eba6f91cf5cace580f442"
   },
   "outputs": [],
   "source": [
    "# First split the data in two sets, 80% for training, 20% for Val/Test)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X,Y1, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 224, 224, 3), (200, 224, 224, 3), (800, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "34977dad-9c51-4946-a465-d79138306b03",
    "_uuid": "c5e3ba21cd44491307721204db024166a5028499"
   },
   "outputs": [],
   "source": [
    "nb_train_samples = len(X_train)\n",
    "nb_validation_samples = len(X_val)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "img_size = 224\n",
    "img_flat_size = img_size * img_size\n",
    "\n",
    "# If you want to train the model -> True, otherwise -> False\n",
    "Is_train = True\n",
    "\n",
    "# If you want to load saved model -> True, otherwise -> False \n",
    "Load_model = False\n",
    "\n",
    "# Name of the save file\n",
    "save_name = 'soft1'\n",
    "\n",
    "# Numbers of sampling to test the code \n",
    "num_test_sample = 10\n",
    "\n",
    "# labels: 0 - 9\n",
    "num_label = 2\n",
    "\n",
    "# Parameters for training\n",
    "num_epoch = 5\n",
    "\n",
    "learning_rate = 1e-4        # lr = 0.0001\n",
    "epsilon = 1e-8\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Parameter for LSTM\n",
    "lstm_size = 256\n",
    "step_size = 4\n",
    "flatten_size = img_size\n",
    "\n",
    "gpu_fraction = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"ResnetModel\"):\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', pooling='max', include_top=False)     #top false. Removed last 2 layers(GlobalAveragePooling2 &dense_prediction) from full model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image  = tf.placeholder(tf.float32, shape = [None, 224, 224, 3])\n",
    "y_target = tf.placeholder(tf.float32, shape=[None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = base_model(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias \n",
    "def conv2d(x,w, stride):\n",
    "\treturn tf.nn.conv2d(x,w,strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Get Variables\n",
    "def weight_variable(name, shape):\n",
    "    return tf.get_variable(name,shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    return tf.get_variable(name,shape=shape, initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fc1 = weight_variable('w_fc8',[2048, 2])\n",
    "b_fc1 = bias_variable('b_fc8', [2])\n",
    "\n",
    "output = tf.matmul(last_layer, w_fc1)+b_fc1\n",
    "output = tf.nn.softmax(output)\n",
    "# Training \n",
    "Loss = tf.keras.backend.categorical_crossentropy(target = y_target, output = output)\n",
    "Cost = tf.reduce_mean(Loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, epsilon = epsilon).minimize(Cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_target,1), tf.argmax(output,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9, 0.999, array([[-0.00094979, -0.03933541],\n",
       "        [ 0.01569667,  0.04805824],\n",
       "        [-0.04523479,  0.04087614],\n",
       "        ...,\n",
       "        [ 0.04599   ,  0.00376365],\n",
       "        [-0.0099079 ,  0.03910299],\n",
       "        [-0.03322377,  0.04828329]], dtype=float32), array([0., 0.], dtype=float32), array([0., 0.], dtype=float32), array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=float32), array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=float32), array([-0.3406089 , -0.39275515], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = gpu_fraction\n",
    "\n",
    "sess = tf.keras.backend.get_session()\n",
    "initialize_variables_list = list(set(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))^set(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='ResnetModel')))\n",
    "writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "sess.run(initialize_variables_list )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / Batch: 0/800 / Cost: 0.02107138 / Training Accuracy: 1.0 / Validation Accuracy: 0.96\n",
      "Epoch: 1 / Batch: 16/800 / Cost: 0.007492285 / Training Accuracy: 1.0 / Validation Accuracy: 0.95\n",
      "Epoch: 1 / Batch: 32/800 / Cost: 0.021728104 / Training Accuracy: 1.0 / Validation Accuracy: 0.95\n",
      "Epoch: 1 / Batch: 48/800 / Cost: 0.037527107 / Training Accuracy: 1.0 / Validation Accuracy: 0.96\n",
      "Epoch: 1 / Batch: 64/800 / Cost: 0.0031573616 / Training Accuracy: 1.0 / Validation Accuracy: 0.965\n",
      "Epoch: 1 / Batch: 80/800 / Cost: 0.003676218 / Training Accuracy: 1.0 / Validation Accuracy: 0.985\n",
      "Epoch: 1 / Batch: 96/800 / Cost: 0.040438753 / Training Accuracy: 1.0 / Validation Accuracy: 0.985\n",
      "Epoch: 1 / Batch: 112/800 / Cost: 0.015245602 / Training Accuracy: 1.0 / Validation Accuracy: 0.94\n",
      "Epoch: 1 / Batch: 128/800 / Cost: 0.03253888 / Training Accuracy: 1.0 / Validation Accuracy: 0.93\n",
      "Epoch: 1 / Batch: 144/800 / Cost: 0.023627542 / Training Accuracy: 1.0 / Validation Accuracy: 0.945\n",
      "Epoch: 1 / Batch: 160/800 / Cost: 0.05249906 / Training Accuracy: 0.9375 / Validation Accuracy: 0.97\n",
      "Epoch: 1 / Batch: 176/800 / Cost: 0.02015049 / Training Accuracy: 1.0 / Validation Accuracy: 0.975\n",
      "Epoch: 1 / Batch: 192/800 / Cost: 0.06360535 / Training Accuracy: 1.0 / Validation Accuracy: 0.98\n",
      "Epoch: 1 / Batch: 208/800 / Cost: 0.11784703 / Training Accuracy: 0.9375 / Validation Accuracy: 0.96\n",
      "Epoch: 1 / Batch: 224/800 / Cost: 0.10443633 / Training Accuracy: 0.9375 / Validation Accuracy: 0.96\n",
      "Epoch: 1 / Batch: 240/800 / Cost: 0.033020105 / Training Accuracy: 1.0 / Validation Accuracy: 0.965\n",
      "Epoch: 1 / Batch: 256/800 / Cost: 0.009830425 / Training Accuracy: 1.0 / Validation Accuracy: 0.975\n",
      "Epoch: 1 / Batch: 272/800 / Cost: 0.018133098 / Training Accuracy: 1.0 / Validation Accuracy: 0.97\n",
      "Epoch: 1 / Batch: 288/800 / Cost: 0.074207544 / Training Accuracy: 0.9375 / Validation Accuracy: 0.935\n",
      "Epoch: 1 / Batch: 304/800 / Cost: 0.093402274 / Training Accuracy: 0.9375 / Validation Accuracy: 0.93\n",
      "Epoch: 1 / Batch: 320/800 / Cost: 0.027079623 / Training Accuracy: 1.0 / Validation Accuracy: 0.945\n",
      "Epoch: 1 / Batch: 336/800 / Cost: 0.03450954 / Training Accuracy: 1.0 / Validation Accuracy: 0.97\n",
      "Epoch: 1 / Batch: 352/800 / Cost: 0.115098506 / Training Accuracy: 0.9375 / Validation Accuracy: 0.975\n",
      "Epoch: 1 / Batch: 368/800 / Cost: 0.10019705 / Training Accuracy: 0.9375 / Validation Accuracy: 0.95\n",
      "Epoch: 1 / Batch: 384/800 / Cost: 0.060831457 / Training Accuracy: 1.0 / Validation Accuracy: 0.945\n",
      "Epoch: 1 / Batch: 400/800 / Cost: 0.10703772 / Training Accuracy: 0.9375 / Validation Accuracy: 0.945\n",
      "Epoch: 1 / Batch: 416/800 / Cost: 0.048003517 / Training Accuracy: 1.0 / Validation Accuracy: 0.925\n",
      "Epoch: 1 / Batch: 432/800 / Cost: 0.08154188 / Training Accuracy: 0.9375 / Validation Accuracy: 0.915\n",
      "Epoch: 1 / Batch: 448/800 / Cost: 0.10763888 / Training Accuracy: 0.9375 / Validation Accuracy: 0.9\n",
      "Epoch: 1 / Batch: 464/800 / Cost: 0.11577727 / Training Accuracy: 1.0 / Validation Accuracy: 0.93\n",
      "Epoch: 1 / Batch: 480/800 / Cost: 0.085473746 / Training Accuracy: 1.0 / Validation Accuracy: 0.94\n",
      "Epoch: 1 / Batch: 496/800 / Cost: 0.14870107 / Training Accuracy: 0.9375 / Validation Accuracy: 0.92\n",
      "Epoch: 1 / Batch: 512/800 / Cost: 0.16693658 / Training Accuracy: 0.875 / Validation Accuracy: 0.9\n",
      "Epoch: 1 / Batch: 528/800 / Cost: 0.15869908 / Training Accuracy: 0.9375 / Validation Accuracy: 0.91\n",
      "Epoch: 1 / Batch: 544/800 / Cost: 0.09656442 / Training Accuracy: 1.0 / Validation Accuracy: 0.945\n",
      "Epoch: 1 / Batch: 560/800 / Cost: 0.10301746 / Training Accuracy: 0.9375 / Validation Accuracy: 0.975\n",
      "Epoch: 1 / Batch: 576/800 / Cost: 0.06328504 / Training Accuracy: 1.0 / Validation Accuracy: 0.99\n",
      "Epoch: 1 / Batch: 592/800 / Cost: 0.03543804 / Training Accuracy: 1.0 / Validation Accuracy: 0.99\n",
      "Epoch: 1 / Batch: 608/800 / Cost: 0.058287956 / Training Accuracy: 1.0 / Validation Accuracy: 0.955\n",
      "Epoch: 1 / Batch: 624/800 / Cost: 0.04091507 / Training Accuracy: 1.0 / Validation Accuracy: 0.99\n",
      "Epoch: 1 / Batch: 640/800 / Cost: 0.009699042 / Training Accuracy: 1.0 / Validation Accuracy: 0.985\n",
      "Epoch: 1 / Batch: 656/800 / Cost: 0.11380568 / Training Accuracy: 0.9375 / Validation Accuracy: 0.985\n",
      "Epoch: 1 / Batch: 672/800 / Cost: 0.015774604 / Training Accuracy: 1.0 / Validation Accuracy: 0.985\n",
      "Epoch: 1 / Batch: 688/800 / Cost: 0.0047674878 / Training Accuracy: 1.0 / Validation Accuracy: 0.985\n",
      "Epoch: 1 / Batch: 704/800 / Cost: 0.016975056 / Training Accuracy: 1.0 / Validation Accuracy: 0.99\n",
      "Epoch: 1 / Batch: 720/800 / Cost: 0.06475173 / Training Accuracy: 0.9375 / Validation Accuracy: 0.99\n",
      "Epoch: 1 / Batch: 736/800 / Cost: 0.004019848 / Training Accuracy: 1.0 / Validation Accuracy: 0.995\n",
      "Epoch: 1 / Batch: 752/800 / Cost: 0.030046897 / Training Accuracy: 1.0 / Validation Accuracy: 0.99\n",
      "Epoch: 1 / Batch: 768/800 / Cost: 0.04326933 / Training Accuracy: 1.0 / Validation Accuracy: 0.975\n",
      "Epoch: 1 / Batch: 784/800 / Cost: 0.0123444 / Training Accuracy: 1.0 / Validation Accuracy: 0.965\n",
      "Model is saved!!!\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "batch_size=16\n",
    "Is_train = True\n",
    "img_size = 224\n",
    "num_epoch = 1\n",
    "if Is_train == True:\n",
    "    train_data_num = X_train.shape[0]\n",
    "\n",
    "    for i in range(num_epoch):\n",
    "        # Making batches\n",
    "        random_idx = np.arange(train_data_num)\n",
    "        np.random.shuffle(random_idx)\n",
    "\n",
    "        batch_count = 1\n",
    "    \n",
    "        for j in range(0, train_data_num, batch_size):\n",
    "            if j + batch_size < train_data_num:\n",
    "                batch_index = [j, j + batch_size]\n",
    "\n",
    "                batch_x_train = X_train[random_idx[batch_index[0]:batch_index[1]],:,:]\n",
    "                batch_y_train = Y_train[random_idx[batch_index[0]:batch_index[1]],:]\n",
    "            else:\n",
    "                batch_index = [j, j + train_data_num-1]\n",
    "\n",
    "                batch_x_train = X_train[random_idx[batch_index[0]:batch_index[-1]],:,:]\n",
    "                batch_y_train = Y_train[random_idx[batch_index[0]:batch_index[-1]],:]\n",
    "\n",
    "            # Make image as fractions for attention\n",
    "            train_batch = np.reshape(batch_x_train, (batch_x_train.shape[0], img_size, img_size, 3))\n",
    "            validation_batch = np.reshape(X_val, (X_val.shape[0], img_size, img_size, 3))\n",
    "            \n",
    "            # Training\n",
    "            optimizer.run(session=sess, feed_dict = {x_image: train_batch, y_target: batch_y_train})\n",
    "            cost = Cost.eval( session=sess, feed_dict = {x_image: train_batch, y_target: batch_y_train})\n",
    "            acc = accuracy.eval( session=sess, feed_dict = {x_image: train_batch, y_target: batch_y_train})\n",
    "            val_acc = accuracy.eval( session=sess, feed_dict = {x_image: X_val, y_target: Y_val})\n",
    "\n",
    "            # Print Progress\n",
    "            print(\"Epoch: \" + str(i+1) + ' / ' + \n",
    "                  \"Batch: \" + str(j) + '/' + str(train_data_num) + ' / ' + \n",
    "                  \"Cost: \" + str(cost) + ' / ' + \n",
    "                  \"Training Accuracy: \" + str(acc) + ' / ' + \n",
    "                  \"Validation Accuracy: \" + str(val_acc))  \n",
    "\n",
    "        print('Model is saved!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
